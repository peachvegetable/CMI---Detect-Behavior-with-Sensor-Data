{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa95356",
   "metadata": {
    "papermill": {
     "duration": 0.006446,
     "end_time": "2025-07-30T08:46:09.581765",
     "exception": false,
     "start_time": "2025-07-30T08:46:09.575319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ğŸ†0.81|ğŸ”¥å®Œæ•´æµç¨‹ï¼šæ•°æ®å¤„ç†+æ¨¡å‹è®­ç»ƒğŸ”¥\n",
    "\n",
    "> **ä½œè€…ï¼š** H-Z-Ning  \n",
    "> **æ—¥æœŸï¼š** 2025-07-29  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ç›®å½•\n",
    "1. âš™ï¸ [åˆå§‹åŒ–](#1-åˆå§‹åŒ–)  \n",
    "2. ğŸ“ˆ [IMU ä¸é™€èºä»ªç‰¹å¾æå–](#2-imu--é™€èºä»ªç‰¹å¾æå–)  \n",
    "3. ğŸ§  [ç®€å• MLP](#3-ç®€å•-mlp)  \n",
    "4. ğŸ§© [CNN + ResNet](#4-cnn--resnet)  \n",
    "5. ğŸ” [æ™®é€šç¥ç»ç½‘ç»œ](#5-æ™®é€šç¥ç»ç½‘ç»œ)  \n",
    "6. ğŸ”€ [åŒåˆ†æ”¯ LSTM-MLP](#6-åŒåˆ†æ”¯-lstm-mlp)  \n",
    "7. ğŸ§¹ [æ•°æ®é›†é¢„å¤„ç†](#7-æ•°æ®é›†é¢„å¤„ç†)  \n",
    "8. ğŸ› ï¸ [è®­ç»ƒå·¥å…·](#8-è®­ç»ƒå·¥å…·)  \n",
    "9. ğŸ² [æ•°æ®å¢å¼º](#9-æ•°æ®å¢å¼º)  \n",
    "10. ğŸ·ï¸ [ç‰¹å¾ä¸æ ‡ç­¾](#10-ç‰¹å¾ä¸æ ‡ç­¾)  \n",
    "11. ğŸŒ [é‡åŠ›åˆ†é‡å»é™¤](#11-é‡åŠ›åˆ†é‡å»é™¤)  \n",
    "12. ğŸ”„ [å››å…ƒæ•° â†’ è§’é€Ÿåº¦ä¸è§’åº¦](#12-å››å…ƒæ•°--è§’é€Ÿåº¦ä¸è§’åº¦)  \n",
    "13. ğŸ“Š [ç‰¹å¾æ‰©å±•](#13-ç‰¹å¾æ‰©å±•)  \n",
    "14. ğŸ§± [ç½‘ç»œå®šä¹‰](#14-ç½‘ç»œå®šä¹‰)  \n",
    "15. ğŸ“¦ [æ•°æ®é›†å®šä¹‰](#15-æ•°æ®é›†å®šä¹‰)  \n",
    "17. ğŸ—³ï¸ [é›†æˆé¢„æµ‹å™¨](#17-é›†æˆé¢„æµ‹å™¨)  \n",
    "18. ğŸš€ [è®­ç»ƒä¸æ¨ç†](#18-è®­ç»ƒä¸æ¨ç†)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. âš™ï¸ åˆå§‹åŒ–\n",
    "è®¾ç½®éšæœºç§å­ã€å…¨å±€è¶…å‚æ•°å’Œå®éªŒç¯å¢ƒï¼Œä»¥ç¡®ä¿ç»“æœå¯å¤ç°ã€‚\n",
    "\n",
    "## 2. ğŸ“ˆ IMU ä¸é™€èºä»ªç‰¹å¾æå–\n",
    "ä»åŸå§‹åŠ é€Ÿåº¦è®¡å’Œé™€èºä»ªä¿¡å·ä¸­æå–ç‰¹å¾ï¼š\n",
    "- åŸå§‹ä¿¡å·  \n",
    "- ä½é€šæ»¤æ³¢åçš„ä¿¡å·  \n",
    "- é«˜é€šæ»¤æ³¢åçš„ä¿¡å·  \n",
    "- å¹…å€¼ï¼ˆMagnitudeï¼‰  \n",
    "- å·®åˆ†ï¼ˆç›¸é‚»æ—¶é—´æ­¥çš„å˜åŒ–é‡ï¼‰  \n",
    "- åŠŸç‡ï¼ˆPowerï¼‰\n",
    "\n",
    "## 3. ğŸ§  ç®€å• MLP\n",
    "ä¸€ä¸ªå…¨è¿æ¥å‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºå¿«é€ŸéªŒè¯ç‰¹å¾æœ‰æ•ˆæ€§ã€‚\n",
    "\n",
    "## 4. ğŸ§© CNN + ResNet\n",
    "ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼ˆ1D-CNNï¼‰ï¼Œå¼•å…¥æ®‹å·®è¿æ¥ï¼Œç”¨ä»¥æ•æ‰å±€éƒ¨æ—¶é—´æ¨¡å¼ã€‚\n",
    "\n",
    "## 5. ğŸ” æ™®é€šç¥ç»ç½‘ç»œ\n",
    "è½»é‡çº§å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œä½œä¸ºåŸºçº¿æ¨¡å‹ä½¿ç”¨ã€‚\n",
    "\n",
    "## 6. ğŸ”€ åŒåˆ†æ”¯ LSTM-MLP\n",
    "- **IMU åˆ†æ”¯**ï¼š3è½´åŠ é€Ÿåº¦è®¡ + 3è½´é™€èºä»ª  \n",
    "- **TOF åˆ†æ”¯**ï¼šé£è¡Œæ—¶é—´ï¼ˆTime-of-Flightï¼‰è·ç¦»æ•°æ®  \n",
    "- **èåˆæ–¹å¼**ï¼šä¸¤ä¸ªåˆ†æ”¯è¾“å‡ºæ‹¼æ¥ â†’ LSTM â†’ MLP\n",
    "\n",
    "## 7. ğŸ§¹ æ•°æ®é›†é¢„å¤„ç†\n",
    "- æ»‘åŠ¨çª—å£åˆ†æ®µ  \n",
    "- æ ‡å‡†åŒ– / å½’ä¸€åŒ–  \n",
    "- è®­ç»ƒé›† / éªŒè¯é›† / æµ‹è¯•é›†åˆ’åˆ†\n",
    "\n",
    "## 8. ğŸ› ï¸ è®­ç»ƒå·¥å…·\n",
    "- æ—©åœæœºåˆ¶ï¼ˆEarly Stoppingï¼‰  \n",
    "- EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰ç”¨äºæƒé‡å¹³æ»‘  \n",
    "- å›ºå®šéšæœºç§å­\n",
    "\n",
    "## 9. ğŸ² æ•°æ®å¢å¼º\n",
    "- éšæœºç¼©æ”¾ã€æ—‹è½¬ã€æ·»åŠ å™ªå£°  \n",
    "- æ—¶é—´åŸŸçš„ Mixup / CutMix å˜ä½“\n",
    "\n",
    "## 10. ğŸ·ï¸ ç‰¹å¾ä¸æ ‡ç­¾\n",
    "ç”Ÿæˆæœ€ç»ˆçš„è®­ç»ƒæ ·æœ¬å¯¹ `(X, y)`ï¼Œå¹¶ä¿å­˜ä¸º `.npz` æˆ– `.tfrecord` æ ¼å¼ã€‚\n",
    "\n",
    "## 11. ğŸŒ é‡åŠ›åˆ†é‡å»é™¤\n",
    "ä»åŠ é€Ÿåº¦è®¡æ•°æ®ä¸­å»é™¤é‡åŠ›å½±å“ï¼Œæ–¹æ³•åŒ…æ‹¬ï¼š\n",
    "- é«˜é€šæ»¤æ³¢  \n",
    "- æ–¹å‘ä½™å¼¦çŸ©é˜µï¼ˆDCMï¼‰æ—‹è½¬\n",
    "\n",
    "## 12. ğŸ”„ å››å…ƒæ•° â†’ è§’é€Ÿåº¦ä¸è§’åº¦\n",
    "- å››å…ƒæ•°å¯¼æ•° â†’ è§’é€Ÿåº¦  \n",
    "- å››å…ƒæ•°è·ç¦» â†’ å§¿æ€å˜åŒ–è§’åº¦\n",
    "\n",
    "## 13. ğŸ“Š ç‰¹å¾æ‰©å±•\n",
    "å°†åŸå§‹çš„ 7 ä¸ªé€šé“ï¼ˆ3è½´åŠ é€Ÿåº¦ + 4è½´å››å…ƒæ•°ï¼‰æ‰©å±•ä¸ºåŒ…å« 22 ä¸ªç‰¹å¾çš„ä¸°å¯Œç‰¹å¾é›†ï¼š\n",
    "\n",
    "| ç¼–å· | ç‰¹å¾ |\n",
    "|------|------|\n",
    "| 0â€“2 | æ— é‡åŠ›åŠ é€Ÿåº¦ï¼šAcc_X, Acc_Y, Acc_Z |\n",
    "| 3â€“5 | é™€èºä»ªï¼šGyro_X, Gyro_Y, Gyro_Z |\n",
    "| 6â€“9 | å››å…ƒæ•°ï¼šw, x, y, z |\n",
    "| 10â€“12 | åŠ é€Ÿåº¦å¹…å€¼åŠå…¶å·®åˆ† |\n",
    "| 13â€“15 | é™€èºä»ªå¹…å€¼åŠå…¶å·®åˆ† |\n",
    "| 16 | å››å…ƒæ•°å¹…å€¼ |\n",
    "| 17â€“21 | æ¬§æ‹‰è§’ï¼ˆRoll, Pitch, Yawï¼‰åŠè§’é€Ÿåº¦æ¨¡é•¿ |\n",
    "\n",
    "## 14. ğŸ§± ç½‘ç»œå®šä¹‰\n",
    "ç»Ÿä¸€æ¥å£æ”¯æŒå¤šç§ç½‘ç»œç»“æ„ï¼Œå¦‚ CNNã€LSTMã€MLPã€Transformer ç­‰ã€‚\n",
    "\n",
    "## 15. ğŸ“¦ æ•°æ®é›†å®šä¹‰\n",
    "å°è£… `tf.data.Dataset` æˆ– `torch.utils.data.Dataset`ï¼Œæ”¯æŒæ‡’åŠ è½½ä¸å¤šçº¿ç¨‹é¢„å¤„ç†ã€‚\n",
    "\n",
    "## 17. ğŸ—³ï¸ é›†æˆé¢„æµ‹å™¨\n",
    "åŠ è½½å¤šä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆä¸åŒæ¶æ„æˆ–äº¤å‰éªŒè¯æŠ˜æ•°ï¼‰ï¼Œé€šè¿‡æŠ•ç¥¨æˆ–åŠ æƒå¹³å‡æ–¹å¼èåˆé¢„æµ‹ç»“æœã€‚\n",
    "\n",
    "## 18. ğŸš€ è®­ç»ƒä¸æ¨ç†\n",
    "ç«¯åˆ°ç«¯è„šæœ¬ç¤ºä¾‹ï¼š\n",
    "```bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78fa8a",
   "metadata": {
    "papermill": {
     "duration": 0.004958,
     "end_time": "2025-07-30T08:46:09.592041",
     "exception": false,
     "start_time": "2025-07-30T08:46:09.587083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook based on https://www.kaggle.com/code/myso1987/cmi3-pyroch-baseline-model-add-aug-folds <br />\n",
    "and https://www.kaggle.com/code/jiazhuang/cmi-imu-only-lstm <br />\n",
    "What I do: Just fuse the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c174ab",
   "metadata": {
    "papermill": {
     "duration": 0.004933,
     "end_time": "2025-07-30T08:46:09.602018",
     "exception": false,
     "start_time": "2025-07-30T08:46:09.597085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. âš™ï¸ åˆå§‹åŒ–\n",
    "- åˆå§‹åŒ–ä¸€äº›è¶…å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80ab692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:09.612957Z",
     "iopub.status.busy": "2025-07-30T08:46:09.612724Z",
     "iopub.status.idle": "2025-07-30T08:46:25.398072Z",
     "shell.execute_reply": "2025-07-30T08:46:25.397226Z"
    },
    "papermill": {
     "duration": 15.792366,
     "end_time": "2025-07-30T08:46:25.399351",
     "exception": false,
     "start_time": "2025-07-30T08:46:09.606985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ imports ready Â· pytorch 2.6.0+cu124 Â· device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "import random, math\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from scipy.signal import firwin\n",
    "from cmi_2025_metric_copy_for_import import CompetitionMetric\n",
    "\n",
    "import polars as pl\n",
    "# Configuration\n",
    "TRAIN = False                     # â† set to True when you want to train\n",
    "RAW_DIR = Path(\"../input/cmi-detect-behavior-with-sensor-data\")\n",
    "PRETRAINED_DIR = Path(\"/kaggle/input/cmi3-models-p\") # used when TRAIN=False\n",
    "EXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\n",
    "BATCH_SIZE = 64\n",
    "PAD_PERCENTILE = 100\n",
    "maxlen = PAD_PERCENTILE\n",
    "LR_INIT = 1e-3\n",
    "WD = 3e-3\n",
    "# MIXUP_ALPHA = 0.4\n",
    "PATIENCE = 40\n",
    "FOLDS = 5\n",
    "random_state = 42\n",
    "epochs_warmup = 20\n",
    "warmup_lr_init = 1.822126131809773e-05\n",
    "lr_min = 3.810323058740104e-09\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"â–¶ imports ready Â· pytorch {torch.__version__} Â· device: {device}\")\n",
    "\n",
    "# ================================\n",
    "# Model Components\n",
    "# ================================\n",
    "mean = torch.tensor([\n",
    "    0,  0, 0, 0, 0,\n",
    "    0,  9.0319e-03,  1.0849e+00, -2.6186e-03,  3.7651e-03,\n",
    "    -5.3660e-03, -2.8177e-03,  1.3318e-03, -1.5876e-04,  6.3495e-01,\n",
    "     6.2877e-01,  6.0607e-01,  6.2142e-01,  6.3808e-01,  6.5420e-01,\n",
    "     7.4102e-03, -3.4159e-03, -7.5237e-03, -2.6034e-02,  2.9704e-02,\n",
    "    -3.1546e-02, -2.0610e-03, -4.6986e-03, -4.7216e-03, -2.6281e-02,\n",
    "     1.5799e-02,  1.0016e-02\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device)         \n",
    "\n",
    "std = torch.tensor([\n",
    "    1, 1, 1, 1, 1, 1, 0.2067, 0.8583, 0.3162,\n",
    "    0.2668, 0.2917, 0.2341, 0.3023, 0.3281, 1.0264, 0.8838, 0.8686, 1.0973,\n",
    "    1.0267, 0.9018, 0.4658, 0.2009, 0.2057, 1.2240, 0.9535, 0.6655, 0.2941,\n",
    "    0.3421, 0.8156, 0.6565, 1.1034, 1.5577\n",
    "], dtype=torch.float32).view(1, -1, 1).to(device) + 1e-8  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4304dd",
   "metadata": {
    "papermill": {
     "duration": 0.004977,
     "end_time": "2025-07-30T08:46:25.409997",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.405020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. ğŸ“ˆ IMU ä¸é™€èºä»ªç‰¹å¾æå–\n",
    "ä»åŸå§‹åŠ é€Ÿåº¦è®¡å’Œé™€èºä»ªä¿¡å·ä¸­æå–ä»¥ä¸‹ç‰¹å¾ï¼š\n",
    "- åŸå§‹ä¿¡å·  \n",
    "- ä½é€šæ»¤æ³¢ä¿¡å·  \n",
    "- é«˜é€šæ»¤æ³¢ä¿¡å·  \n",
    "- æ¨¡å€¼ï¼ˆMagnitudeï¼‰  \n",
    "- æ­¥é—´å˜åŒ–é‡ï¼ˆDeltaï¼‰  \n",
    "- åŠŸç‡ï¼ˆPowerï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fbb64c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.421146Z",
     "iopub.status.busy": "2025-07-30T08:46:25.420747Z",
     "iopub.status.idle": "2025-07-30T08:46:25.428541Z",
     "shell.execute_reply": "2025-07-30T08:46:25.428029Z"
    },
    "papermill": {
     "duration": 0.014589,
     "end_time": "2025-07-30T08:46:25.429638",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.415049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImuFeatureExtractor(nn.Module):\n",
    "    def __init__(self, fs=100., add_quaternion=False):\n",
    "        super().__init__()\n",
    "        self.fs = fs\n",
    "        self.add_quaternion = add_quaternion\n",
    "\n",
    "        k = 15\n",
    "        self.lpf = nn.Conv1d(6, 6, kernel_size=k, padding=k//2,\n",
    "                             groups=6, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.lpf.weight, a=math.sqrt(5))\n",
    "\n",
    "        self.lpf_acc  = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "        self.lpf_gyro = nn.Conv1d(3, 3, k, padding=k//2, groups=3, bias=False)\n",
    "\n",
    "    def forward(self, imu):\n",
    "        # imu: \n",
    "        B, C, T = imu.shape\n",
    "        acc  = imu[:, 0:3, :]                 # acc_x, acc_y, acc_z\n",
    "        gyro = imu[:, 3:6, :]                 # gyro_x, gyro_y, gyro_z\n",
    "        extra = imu[:, 6:, :]                 \n",
    "\n",
    "        # 1) magnitude\n",
    "        acc_mag  = torch.norm(acc,  dim=1, keepdim=True)          # (B,1,T)\n",
    "        gyro_mag = torch.norm(gyro, dim=1, keepdim=True)\n",
    "\n",
    "        # 2) jerk \n",
    "        jerk = F.pad(acc[:, :, 1:] - acc[:, :, :-1], (1,0))       # (B,3,T)\n",
    "        gyro_delta = F.pad(gyro[:, :, 1:] - gyro[:, :, :-1], (1,0))\n",
    "\n",
    "        # 3) energy\n",
    "        acc_pow  = acc ** 2\n",
    "        gyro_pow = gyro ** 2\n",
    "\n",
    "        # 4) LPF / HPF \n",
    "        acc_lpf  = self.lpf_acc(acc)\n",
    "        acc_hpf  = acc - acc_lpf\n",
    "        gyro_lpf = self.lpf_gyro(gyro)\n",
    "        gyro_hpf = gyro - gyro_lpf\n",
    "\n",
    "        features = [\n",
    "            acc, gyro,\n",
    "            acc_mag, gyro_mag,\n",
    "            jerk, gyro_delta,\n",
    "            acc_pow, gyro_pow,\n",
    "            acc_lpf, acc_hpf,\n",
    "            gyro_lpf, gyro_hpf,\n",
    "        ]\n",
    "        return torch.cat(features, dim=1)  # (B, C_out, T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db400670",
   "metadata": {
    "papermill": {
     "duration": 0.004874,
     "end_time": "2025-07-30T08:46:25.439655",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.434781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. ğŸ§  ç®€å•ç¥ç»ç½‘ç»œå®šä¹‰ï¼ˆMLPï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44577c12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.450747Z",
     "iopub.status.busy": "2025-07-30T08:46:25.450510Z",
     "iopub.status.idle": "2025-07-30T08:46:25.455071Z",
     "shell.execute_reply": "2025-07-30T08:46:25.454404Z"
    },
    "papermill": {
     "duration": 0.011275,
     "end_time": "2025-07-30T08:46:25.456198",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.444923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46a3c1",
   "metadata": {
    "papermill": {
     "duration": 0.004943,
     "end_time": "2025-07-30T08:46:25.466100",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.461157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. ğŸ§© CNNç½‘ç»œå’Œæ®‹å·®ç½‘ç»œ ï¼ˆCNN + ResNetï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1f6cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.476791Z",
     "iopub.status.busy": "2025-07-30T08:46:25.476566Z",
     "iopub.status.idle": "2025-07-30T08:46:25.482629Z",
     "shell.execute_reply": "2025-07-30T08:46:25.481954Z"
    },
    "papermill": {
     "duration": 0.012527,
     "end_time": "2025-07-30T08:46:25.483653",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.471126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First conv block\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Second conv block\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # SE block\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        # First conv\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Second conv\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # SE block\n",
    "        out = self.se(out)\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Pool and dropout\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d17cd",
   "metadata": {
    "papermill": {
     "duration": 0.004815,
     "end_time": "2025-07-30T08:46:25.493961",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.489146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. ğŸ” æ™®é€šç¥ç»ç½‘ç»œï¼ˆPlain Neural Networkï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f733689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.504786Z",
     "iopub.status.busy": "2025-07-30T08:46:25.504553Z",
     "iopub.status.idle": "2025-07-30T08:46:25.508784Z",
     "shell.execute_reply": "2025-07-30T08:46:25.508086Z"
    },
    "papermill": {
     "duration": 0.010789,
     "end_time": "2025-07-30T08:46:25.509796",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.499007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.tanh(self.attention(x))  # (batch, seq_len, 1)\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)  # (batch, seq_len)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6440ac",
   "metadata": {
    "papermill": {
     "duration": 0.004893,
     "end_time": "2025-07-30T08:46:25.519559",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.514666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. ğŸ”€ åŒåˆ†æ”¯ LSTM-MLP\n",
    "- **IMU åˆ†æ”¯**ï¼š3 è½´åŠ é€Ÿåº¦è®¡ + 3 è½´é™€èºä»ª  \n",
    "- **TOF åˆ†æ”¯**ï¼šTime-of-Flight è·ç¦»æ•°æ®  \n",
    "- **èåˆ**ï¼šåŒåˆ†æ”¯ç‰¹å¾æ‹¼æ¥ â†’ LSTM â†’ MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ed2148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.530497Z",
     "iopub.status.busy": "2025-07-30T08:46:25.530283Z",
     "iopub.status.idle": "2025-07-30T08:46:25.543302Z",
     "shell.execute_reply": "2025-07-30T08:46:25.542619Z"
    },
    "papermill": {
     "duration": 0.01979,
     "end_time": "2025-07-30T08:46:25.544350",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.524560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TwoBranchModel(nn.Module):\n",
    "    def __init__(self, pad_len, imu_dim_raw, tof_dim, n_classes, dropouts=[0.3, 0.3, 0.3, 0.3, 0.4, 0.5, 0.3], feature_engineering=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.feature_engineering = feature_engineering\n",
    "        if feature_engineering:\n",
    "            self.imu_fe = ImuFeatureExtractor(**kwargs)\n",
    "            imu_dim = 32            \n",
    "        else:\n",
    "            self.imu_fe = nn.Identity()\n",
    "            imu_dim = imu_dim_raw   \n",
    "            \n",
    "        self.imu_dim = imu_dim\n",
    "        self.tof_dim = tof_dim\n",
    "\n",
    "        self.fir_nchan = 7\n",
    "\n",
    "        weight_decay = 3e-3\n",
    "\n",
    "        numtaps = 33  \n",
    "        fir_coef = firwin(numtaps, cutoff=1.0, fs=10.0, pass_zero=False)\n",
    "        fir_kernel = torch.tensor(fir_coef, dtype=torch.float32).view(1, 1, -1)\n",
    "        fir_kernel = fir_kernel.repeat(7, 1, 1)  # (imu_dim, 1, numtaps)\n",
    "        self.register_buffer(\"fir_kernel\", fir_kernel)\n",
    "        \n",
    "        # IMU deep branch\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=dropouts[0], weight_decay=weight_decay)\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=dropouts[1], weight_decay=weight_decay)\n",
    "        \n",
    "        # TOF/Thermal lighter branch\n",
    "        self.tof_conv1 = nn.Conv1d(tof_dim, 64, 3, padding=1, bias=False)\n",
    "        self.tof_bn1 = nn.BatchNorm1d(64)\n",
    "        self.tof_pool1 = nn.MaxPool1d(2)\n",
    "        self.tof_drop1 = nn.Dropout(dropouts[2])\n",
    "        \n",
    "        self.tof_conv2 = nn.Conv1d(64, 128, 3, padding=1, bias=False)\n",
    "        self.tof_bn2 = nn.BatchNorm1d(128)\n",
    "        self.tof_pool2 = nn.MaxPool1d(2)\n",
    "        self.tof_drop2 = nn.Dropout(dropouts[3])\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(256, 128, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropouts[4])\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = AttentionLayer(256)  # 128*2 for bidirectional\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense1 = nn.Linear(256, 256, bias=False)\n",
    "        self.bn_dense1 = nn.BatchNorm1d(256)\n",
    "        self.drop1 = nn.Dropout(dropouts[5])\n",
    "        \n",
    "        self.dense2 = nn.Linear(256, 128, bias=False)\n",
    "        self.bn_dense2 = nn.BatchNorm1d(128)\n",
    "        self.drop2 = nn.Dropout(dropouts[6])\n",
    "        \n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input\n",
    "        \n",
    "        imu = x[:, :, :self.fir_nchan].transpose(1, 2)  # (batch, imu_dim, seq_len)\n",
    "        tof = x[:, :, self.fir_nchan:].transpose(1, 2)  # (batch, tof_dim, seq_len)\n",
    "\n",
    "        imu = self.imu_fe(imu)   # (B, imu_dim, T)\n",
    "        filtered = F.conv1d(\n",
    "            imu[:, :self.fir_nchan, :],        # (B,7,T)\n",
    "            self.fir_kernel,\n",
    "            padding=self.fir_kernel.shape[-1] // 2,\n",
    "            groups=self.fir_nchan,\n",
    "        )\n",
    "        \n",
    "        imu = torch.cat([filtered, imu[:, self.fir_nchan:, :]], dim=1)  \n",
    "        imu = (imu - mean) / std \n",
    "        # IMU branch\n",
    "        x1 = self.imu_block1(imu)\n",
    "        x1 = self.imu_block2(x1)\n",
    "        \n",
    "        # TOF branch\n",
    "        x2 = F.relu(self.tof_bn1(self.tof_conv1(tof)))\n",
    "        x2 = self.tof_drop1(self.tof_pool1(x2))\n",
    "        x2 = F.relu(self.tof_bn2(self.tof_conv2(x2)))\n",
    "        x2 = self.tof_drop2(self.tof_pool2(x2))\n",
    "        \n",
    "        # Concatenate branches\n",
    "        merged = torch.cat([x1, x2], dim=1).transpose(1, 2)  # (batch, seq_len, 256)\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_out, _ = self.bilstm(merged)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "        \n",
    "        # Attention\n",
    "        attended = self.attention(lstm_out)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = F.relu(self.bn_dense1(self.dense1(attended)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn_dense2(self.dense2(x)))\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = (self.classifier(x))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ef131",
   "metadata": {
    "papermill": {
     "duration": 0.004911,
     "end_time": "2025-07-30T08:46:25.554217",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.549306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. ğŸ§¹ æ•°æ®é›†é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df44ea77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.565412Z",
     "iopub.status.busy": "2025-07-30T08:46:25.564827Z",
     "iopub.status.idle": "2025-07-30T08:46:25.573217Z",
     "shell.execute_reply": "2025-07-30T08:46:25.572740Z"
    },
    "papermill": {
     "duration": 0.015029,
     "end_time": "2025-07-30T08:46:25.574226",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.559197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Data Handling\n",
    "# ================================\n",
    "\n",
    "def pad_sequences_torch(sequences, maxlen, padding='post', truncating='post', value=0.0):\n",
    "    \"\"\"PyTorch equivalent of Keras pad_sequences\"\"\"\n",
    "    result = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) >= maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:  # 'pre'\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - len(seq)\n",
    "            if padding == 'post':\n",
    "                seq = np.concatenate([seq, np.full((pad_len, seq.shape[1]), value)])\n",
    "            else:  # 'pre'\n",
    "                seq = np.concatenate([np.full((pad_len, seq.shape[1]), value), seq])\n",
    "        result.append(seq)\n",
    "    return np.array(result, dtype=np.float32)\n",
    "\n",
    "def preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list, scaler: StandardScaler):\n",
    "    \"\"\"Normalizes and cleans the time series sequence\"\"\"\n",
    "    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n",
    "    return scaler.transform(mat).astype('float32')\n",
    "\n",
    "class CMI3Dataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 X_list,\n",
    "                 y_list,\n",
    "                 maxlen,\n",
    "                 mode=\"train\",\n",
    "                 imu_dim=7,\n",
    "                 augment=None):\n",
    "        self.X_list = X_list\n",
    "        self.mode = mode\n",
    "        self.y_list = y_list\n",
    "        self.maxlen = maxlen\n",
    "        self.imu_dim = imu_dim     \n",
    "        self.augment = augment   \n",
    "\n",
    "    def pad_sequences_torch(self, seq, maxlen, padding='post', truncating='post', value=0.0):\n",
    "\n",
    "        if seq.shape[0] >= maxlen:\n",
    "            if truncating == 'post':\n",
    "                seq = seq[:maxlen]\n",
    "            else:  # 'pre'\n",
    "                seq = seq[-maxlen:]\n",
    "        else:\n",
    "            pad_len = maxlen - seq.shape[0]\n",
    "            if padding == 'post':\n",
    "                seq = np.concatenate([seq, np.full((pad_len, seq.shape[1]), value)])\n",
    "            else:  # 'pre'\n",
    "                seq = np.concatenate([np.full((pad_len, seq.shape[1]), value), seq])\n",
    "        return seq  \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X_list[index]\n",
    "        y = self.y_list[index]\n",
    "\n",
    "        # ---------- (A)  Augmentation ----------\n",
    "        if self.mode == \"train\" and self.augment is not None:\n",
    "            X = self.augment(X, self.imu_dim)     \n",
    "\n",
    "        X = self.pad_sequences_torch(X, self.maxlen, 'pre', 'pre')\n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661272c",
   "metadata": {
    "papermill": {
     "duration": 0.004865,
     "end_time": "2025-07-30T08:46:25.584243",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.579378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. ğŸ› ï¸ è®­ç»ƒè®¾ç½®\n",
    "- æ—©åœï¼ˆEarly stoppingï¼‰\n",
    "- EMAï¼ˆæŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼‰æƒé‡å¹³æ»‘\n",
    "- å›ºå®šéšæœºç§å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6baca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.595580Z",
     "iopub.status.busy": "2025-07-30T08:46:25.595349Z",
     "iopub.status.idle": "2025-07-30T08:46:25.604286Z",
     "shell.execute_reply": "2025-07-30T08:46:25.603727Z"
    },
    "papermill": {
     "duration": 0.016012,
     "end_time": "2025-07-30T08:46:25.605353",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.589341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "    # torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5908b7d",
   "metadata": {
    "papermill": {
     "duration": 0.004841,
     "end_time": "2025-07-30T08:46:25.615326",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.610485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. ğŸ² æ•°æ®å¢å¼º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ebf8dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.626583Z",
     "iopub.status.busy": "2025-07-30T08:46:25.626108Z",
     "iopub.status.idle": "2025-07-30T08:46:25.633294Z",
     "shell.execute_reply": "2025-07-30T08:46:25.632823Z"
    },
    "papermill": {
     "duration": 0.013976,
     "end_time": "2025-07-30T08:46:25.634360",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.620384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Augment:\n",
    "    def __init__(self,\n",
    "                 p_jitter=0.8, sigma=0.02, scale_range=[0.9,1.1],\n",
    "                 p_dropout=0.3,\n",
    "                 p_moda=0.5,          \n",
    "                 drift_std=0.005,     \n",
    "                 drift_max=0.25):      \n",
    "        self.p_jitter  = p_jitter\n",
    "        self.sigma     = sigma\n",
    "        self.scale_min, self.scale_max = scale_range\n",
    "        self.p_dropout = p_dropout\n",
    "        self.p_moda    = p_moda\n",
    "        self.drift_std = drift_std\n",
    "        self.drift_max = drift_max\n",
    "\n",
    "    # ---------- Jitter & Scaling ----------\n",
    "    def jitter_scale(self, x: np.ndarray) -> np.ndarray:\n",
    "        noise  = np.random.randn(*x.shape) * self.sigma\n",
    "        scale  = np.random.uniform(self.scale_min,\n",
    "                                   self.scale_max,\n",
    "                                   size=(1, x.shape[1]))\n",
    "        return (x + noise) * scale\n",
    "\n",
    "    # ---------- Sensor Drop-out ----------\n",
    "    def sensor_dropout(self,\n",
    "                       x: np.ndarray,\n",
    "                       imu_dim: int) -> np.ndarray:\n",
    "\n",
    "        if random.random() < self.p_dropout:\n",
    "            x[:, imu_dim:] = 0.0\n",
    "        return x\n",
    "\n",
    "    def motion_drift(self, x: np.ndarray, imu_dim: int) -> np.ndarray:\n",
    "\n",
    "        T = x.shape[0]\n",
    "\n",
    "        drift = np.cumsum(\n",
    "            np.random.normal(scale=self.drift_std, size=(T, 1)),\n",
    "            axis=0\n",
    "        )\n",
    "        drift = np.clip(drift, -self.drift_max, self.drift_max)   \n",
    "\n",
    "        x[:, :6] += drift\n",
    "\n",
    "        if imu_dim > 6:\n",
    "            x[:, 6:imu_dim] += drift     \n",
    "        return x\n",
    "    \n",
    "    # ---------- master call ----------\n",
    "    def __call__(self,\n",
    "                 x: np.ndarray,\n",
    "                 imu_dim: int) -> np.ndarray:\n",
    "        if random.random() < self.p_jitter:\n",
    "            x = self.jitter_scale(x)\n",
    "\n",
    "        if random.random() < self.p_moda:\n",
    "            x = self.motion_drift(x, imu_dim)\n",
    "\n",
    "        x = self.sensor_dropout(x, imu_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5fcbe",
   "metadata": {
    "papermill": {
     "duration": 0.00496,
     "end_time": "2025-07-30T08:46:25.644546",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.639586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. ğŸ·ï¸ ç‰¹å¾ä¸æ ‡ç­¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c34c1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.655465Z",
     "iopub.status.busy": "2025-07-30T08:46:25.655238Z",
     "iopub.status.idle": "2025-07-30T08:46:25.659992Z",
     "shell.execute_reply": "2025-07-30T08:46:25.659324Z"
    },
    "papermill": {
     "duration": 0.011428,
     "end_time": "2025-07-30T08:46:25.661083",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.649655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "FEATURE_NAMES = [\n",
    "    'acc_x', 'acc_y', 'acc_z',\n",
    "    'rot_w', 'rot_x', 'rot_y', 'rot_z',\n",
    "    'acc_mag', 'rot_angle', 'acc_mag_jerk', 'rot_angle_vel',\n",
    "    'linear_acc_x', 'linear_acc_y', 'linear_acc_z', 'linear_acc_mag', 'linear_acc_mag_jerk',\n",
    "    'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n",
    "    'angular_distance',\n",
    "]\n",
    "CATEGORICAL_FEATURES = []\n",
    "NUMERICAL_FEATURES = [f for f in FEATURE_NAMES if f not in CATEGORICAL_FEATURES]\n",
    "LABEL_NAMES = [\n",
    "    'Forehead - pull hairline',\n",
    "    'Neck - pinch skin',\n",
    "    'Forehead - scratch',\n",
    "    'Eyelash - pull hair',\n",
    "    'Text on phone',\n",
    "    'Eyebrow - pull hair',\n",
    "    'Neck - scratch',\n",
    "    'Above ear - pull hair',\n",
    "    'Cheek - pinch skin',\n",
    "    'Wave hello',\n",
    "    'Write name in air',\n",
    "    'Pull air toward your face',\n",
    "    'Feel around in tray and pull out an object',\n",
    "    'Write name on leg',\n",
    "    'Pinch knee/leg skin',\n",
    "    'Scratch knee/leg skin',\n",
    "    'Drink from bottle/cup',\n",
    "    'Glasses on/off'\n",
    "]\n",
    "IDX2LABEL = {x: i for i, x in enumerate(LABEL_NAMES)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafdf89e",
   "metadata": {
    "papermill": {
     "duration": 0.004922,
     "end_time": "2025-07-30T08:46:25.671041",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.666119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. ğŸŒ å»é™¤é‡åŠ›å½±å“\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92ccc03d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.682603Z",
     "iopub.status.busy": "2025-07-30T08:46:25.682427Z",
     "iopub.status.idle": "2025-07-30T08:46:25.688275Z",
     "shell.execute_reply": "2025-07-30T08:46:25.687602Z"
    },
    "papermill": {
     "duration": 0.013479,
     "end_time": "2025-07-30T08:46:25.689542",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.676063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    \n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fc8f9",
   "metadata": {
    "papermill": {
     "duration": 0.004934,
     "end_time": "2025-07-30T08:46:25.699416",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.694482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. ğŸ”„ å››å…ƒæ•° â†’ è§’é€Ÿåº¦ä¸è§’åº¦\n",
    "- å››å…ƒæ•°å¯¼æ•° â†’ è§’é€Ÿåº¦\n",
    "- å››å…ƒæ•°è·ç¦» â†’ å§¿æ€å˜åŒ–è§’åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04dea936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.710577Z",
     "iopub.status.busy": "2025-07-30T08:46:25.710117Z",
     "iopub.status.idle": "2025-07-30T08:46:25.718149Z",
     "shell.execute_reply": "2025-07-30T08:46:25.717596Z"
    },
    "papermill": {
     "duration": 0.014684,
     "end_time": "2025-07-30T08:46:25.719156",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.704472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "\n",
    "            # Calculate the relative rotation\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            \n",
    "            # Convert delta rotation to angular velocity vector\n",
    "            # The rotation vector (Euler axis * angle) scaled by 1/dt\n",
    "            # is a good approximation for small delta_rot\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            # If quaternion is invalid, angular velocity remains zero\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0 # Ğ˜Ğ»Ğ¸ np.nan, Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¶ĞµĞ»Ğ°ĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ¸Ñ\n",
    "            continue\n",
    "        try:\n",
    "            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ²Ğ°Ñ‚ĞµÑ€Ğ½Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Rotation\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "\n",
    "            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ: 2 * arccos(|real(p * q*)|)\n",
    "            # Ğ³Ğ´Ğµ p* - ÑĞ¾Ğ¿Ñ€ÑĞ¶ĞµĞ½Ğ½Ñ‹Ğ¹ ĞºĞ²Ğ°Ñ‚ĞµÑ€Ğ½Ğ¸Ğ¾Ğ½ q\n",
    "            # Ğ’ scipy.spatial.transform.Rotation, r1.inv() * r2 Ğ´Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ.\n",
    "            # Ğ£Ğ³Ğ¾Ğ» ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ - ÑÑ‚Ğ¾ Ğ¸ ĞµÑÑ‚ÑŒ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğµ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ.\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            \n",
    "            # Ğ£Ğ³Ğ¾Ğ» rotation vector ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ ÑƒĞ³Ğ»Ğ¾Ğ²Ğ¾Ğ¼Ñƒ Ñ€Ğ°ÑÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ\n",
    "            # ĞĞ¾Ñ€Ğ¼Ğ° rotation vector - ÑÑ‚Ğ¾ ÑƒĞ³Ğ¾Ğ» Ğ² Ñ€Ğ°Ğ´Ğ¸Ğ°Ğ½Ğ°Ñ…\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0 # Ğ’ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ½ĞµĞ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… ĞºĞ²Ğ°Ñ‚ĞµÑ€Ğ½Ğ¸Ğ¾Ğ½Ğ¾Ğ²\n",
    "            pass\n",
    "            \n",
    "    return angular_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0fddd",
   "metadata": {
    "papermill": {
     "duration": 0.005058,
     "end_time": "2025-07-30T08:46:25.729380",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.724322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 13. ğŸ“Š ç‰¹å¾æ‰©å±•\n",
    "å°†åŸå§‹çš„ 7 ä¸ªé€šé“ï¼ˆ3 è½´åŠ é€Ÿåº¦ + 4 è½´å››å…ƒæ•°ï¼‰æ‰©å±•ä¸º 22 ç»´ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ffacb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.740421Z",
     "iopub.status.busy": "2025-07-30T08:46:25.740238Z",
     "iopub.status.idle": "2025-07-30T08:46:25.749023Z",
     "shell.execute_reply": "2025-07-30T08:46:25.748299Z"
    },
    "papermill": {
     "duration": 0.015618,
     "end_time": "2025-07-30T08:46:25.750039",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.734421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(train_df):\n",
    "    train_df['acc_mag'] = np.sqrt(train_df['acc_x']**2 + train_df['acc_y']**2 + train_df['acc_z']**2)\n",
    "    train_df['rot_angle'] = 2 * np.arccos(train_df['rot_w'].clip(-1, 1))\n",
    "    train_df['acc_mag_jerk'] = train_df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\n",
    "    train_df['rot_angle_vel'] = train_df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n",
    "\n",
    "    def get_linear_accel(df):\n",
    "        res = remove_gravity_from_acc(\n",
    "            df[['acc_x', 'acc_y', 'acc_z']],\n",
    "            df[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n",
    "        )\n",
    "        return pd.DataFrame(res, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=df.index)\n",
    "\n",
    "    linear_accel_df = train_df.groupby('sequence_id').apply(get_linear_accel, include_groups=False).droplevel(0)\n",
    "    train_df = train_df.join(linear_accel_df)\n",
    "    train_df['linear_acc_mag'] = np.sqrt(train_df['linear_acc_x']**2 + train_df['linear_acc_y']**2 + train_df['linear_acc_z']**2)\n",
    "    train_df['linear_acc_mag_jerk'] = train_df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n",
    "\n",
    "    def calc_angular_velocity(df):\n",
    "        res = calculate_angular_velocity_from_quat(df[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "        return pd.DataFrame(res, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=df.index)\n",
    "\n",
    "    angular_velocity_df = train_df.groupby('sequence_id').apply(calc_angular_velocity, include_groups=False).droplevel(0)\n",
    "    train_df = train_df.join(angular_velocity_df)\n",
    "\n",
    "    train_df['angular_jerk_x'] = train_df.groupby('sequence_id')['angular_vel_x'].diff().fillna(0)\n",
    "    train_df['angular_jerk_y'] = train_df.groupby('sequence_id')['angular_vel_y'].diff().fillna(0)\n",
    "    train_df['angular_jerk_z'] = train_df.groupby('sequence_id')['angular_vel_z'].diff().fillna(0)\n",
    "    train_df['angular_snap_x'] = train_df.groupby('sequence_id')['angular_jerk_x'].diff().fillna(0)\n",
    "    train_df['angular_snap_y'] = train_df.groupby('sequence_id')['angular_jerk_y'].diff().fillna(0)\n",
    "    train_df['angular_snap_z'] = train_df.groupby('sequence_id')['angular_jerk_z'].diff().fillna(0)\n",
    "\n",
    "    def calc_angular_distance(df):\n",
    "        res = calculate_angular_distance(df[['rot_x', 'rot_y', 'rot_z', 'rot_w']])\n",
    "        return pd.DataFrame(res, columns=['angular_distance'], index=df.index)\n",
    "\n",
    "    angular_distance_df = train_df.groupby('sequence_id').apply(calc_angular_distance, include_groups=False).droplevel(0)\n",
    "    train_df = train_df.join(angular_distance_df)\n",
    "    train_df[FEATURE_NAMES] = train_df[FEATURE_NAMES].ffill().bfill().fillna(0).values.astype('float32')\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df2097",
   "metadata": {
    "papermill": {
     "duration": 0.004759,
     "end_time": "2025-07-30T08:46:25.759815",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.755056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 14. ğŸ§± ç½‘ç»œå®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6182885a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.770784Z",
     "iopub.status.busy": "2025-07-30T08:46:25.770569Z",
     "iopub.status.idle": "2025-07-30T08:46:25.783862Z",
     "shell.execute_reply": "2025-07-30T08:46:25.783345Z"
    },
    "papermill": {
     "duration": 0.02009,
     "end_time": "2025-07-30T08:46:25.784807",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.764717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool1d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSECNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, pool_size=2, dropout=0.3, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First conv block\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Second conv block\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # SE block\n",
    "        self.se = SEBlock(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            )\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        # First conv\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Second conv\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # SE block\n",
    "        out = self.se(out)\n",
    "        \n",
    "        # Add shortcut\n",
    "        out += shortcut\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Pool and dropout\n",
    "        out = self.pool(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, hidden_dim)\n",
    "        scores = torch.tanh(self.attention(x))  # (batch, seq_len, 1)\n",
    "        weights = F.softmax(scores.squeeze(-1), dim=1)  # (batch, seq_len)\n",
    "        context = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (batch, hidden_dim)\n",
    "        return context\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, imu_dim, tof_dim, n_classes, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.imu_dim = imu_dim\n",
    "        self.tof_dim = tof_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # IMU deep branch\n",
    "        self.imu_block1 = ResidualSECNNBlock(imu_dim, 64, 3, dropout=0.3, weight_decay=weight_decay)\n",
    "        self.imu_block2 = ResidualSECNNBlock(64, 128, 5, dropout=0.3, weight_decay=weight_decay)\n",
    "\n",
    "        self.bigru = nn.GRU(128, 128, bidirectional=True, batch_first=True)\n",
    "        self.gru_dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = AttentionLayer(256)  # 128*2 for bidirectional\n",
    "        \n",
    "        # Dense layers\n",
    "        self.dense1 = nn.Linear(256, 256, bias=False)\n",
    "        self.bn_dense1 = nn.BatchNorm1d(256)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.dense2 = nn.Linear(256, 128, bias=False)\n",
    "        self.bn_dense2 = nn.BatchNorm1d(128)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.classifier = nn.Linear(128, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, total_dim)\n",
    "        imu = x[:, :, :self.imu_dim]          # (B, T, C)\n",
    "        imu = imu.transpose(1, 2)             # â†’ (B, C, T)\n",
    "    \n",
    "        # CNN blocks\n",
    "        x = self.imu_block1(imu)              # (B, C', T')\n",
    "        x = self.imu_block2(x)                # (B, C'', T'')\n",
    "        x = x.transpose(1, 2)                 # (B, T'', C'')\n",
    "    \n",
    "        # GRU\n",
    "        gru_out, _ = self.bigru(x)            # (B, T'', 2H)\n",
    "        gru_out = self.gru_dropout(gru_out)\n",
    "    \n",
    "        # Attention\n",
    "        attended = self.attention(gru_out)    # (B, 2H)\n",
    "    \n",
    "        # Dense layers\n",
    "        x = F.relu(self.bn_dense1(self.dense1(attended)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn_dense2(self.dense2(x)))\n",
    "        x = self.drop2(x)\n",
    "    \n",
    "        # Classifier\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e2d8ac",
   "metadata": {
    "papermill": {
     "duration": 0.004929,
     "end_time": "2025-07-30T08:46:25.794879",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.789950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 15. ğŸ“¦ æ•°æ®é›†å‡†å¤‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe02cb05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.805994Z",
     "iopub.status.busy": "2025-07-30T08:46:25.805550Z",
     "iopub.status.idle": "2025-07-30T08:46:25.811220Z",
     "shell.execute_reply": "2025-07-30T08:46:25.810729Z"
    },
    "papermill": {
     "duration": 0.012181,
     "end_time": "2025-07-30T08:46:25.812207",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.800026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences: List[List[Dict]], labels: List[int]):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.processed_sequences = self._process_sequences()\n",
    "        \n",
    "        \n",
    "    def _process_sequences(self):\n",
    "        processed = []\n",
    "        \n",
    "        for seq in self.sequences:\n",
    "            seq = seq[:MAX_SEQ_LENGTH]\n",
    "            \n",
    "            seq_array = []\n",
    "            for timestep in seq:\n",
    "                features = [timestep[feature] for feature in FEATURE_NAMES]\n",
    "                seq_array.append(features)\n",
    "            processed.append(np.array(seq_array, dtype=np.float32))\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.FloatTensor(self.processed_sequences[idx])\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1d743",
   "metadata": {
    "papermill": {
     "duration": 0.004949,
     "end_time": "2025-07-30T08:46:25.822274",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.817325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 17. ğŸ—³ï¸ é›†æˆé¢„æµ‹å™¨\n",
    "åŠ è½½å¤šä¸ªå·²è®­ç»ƒæ¨¡å‹ï¼ˆä¸åŒæ¶æ„æˆ–äº¤å‰éªŒè¯æŠ˜ï¼‰ï¼Œé€šè¿‡æŠ•ç¥¨æˆ–åŠ æƒå¹³å‡æ±‡æ€»é¢„æµ‹ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9f93f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.833010Z",
     "iopub.status.busy": "2025-07-30T08:46:25.832823Z",
     "iopub.status.idle": "2025-07-30T08:46:25.839243Z",
     "shell.execute_reply": "2025-07-30T08:46:25.838741Z"
    },
    "papermill": {
     "duration": 0.012951,
     "end_time": "2025-07-30T08:46:25.840193",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.827242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import polars as pl\n",
    "\n",
    "import joblib\n",
    "\n",
    "class EnsemblePredictor:\n",
    "    \n",
    "    def __init__(self, models_dir, device='cpu'):\n",
    "        self.device = device\n",
    "        self.models = []\n",
    "        self._load_models(models_dir)\n",
    "    \n",
    "    def _load_models(self, models_dir):\n",
    "        model_files = sorted(glob.glob(f\"{models_dir}/fold_*_model.pth\"))\n",
    "        print(f\"ğŸ“ æ‰¾åˆ° {len(model_files)} ä¸ªfoldæ¨¡å‹\")\n",
    "        \n",
    "        for model_file in model_files:\n",
    "            checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)\n",
    "            config = checkpoint['model_config']\n",
    "            \n",
    "            model = LSTM(**config)\n",
    "            \n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.to(self.device)\n",
    "            model.eval()\n",
    "            self.models.append(model)\n",
    "            \n",
    "            print(f\"âœ… åŠ è½½ {model_file}\")\n",
    "        \n",
    "    def predict(self, sequence: List[Dict]) -> str:\n",
    "        predictions = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            dataset = SequenceDataset([sequence], [0])\n",
    "            \n",
    "            processed_sequence = torch.FloatTensor(dataset.processed_sequences[0]).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(processed_sequence)\n",
    "                predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "                predictions.append(predicted_class)\n",
    "        \n",
    "        most_common_prediction = Counter(predictions).most_common(1)[0][0]\n",
    "        \n",
    "        predicted_label = LABEL_NAMES[most_common_prediction]\n",
    "        \n",
    "        return predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5528e",
   "metadata": {
    "papermill": {
     "duration": 0.004934,
     "end_time": "2025-07-30T08:46:25.850249",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.845315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 18. ğŸš€ è®­ç»ƒä¸æ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f987ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T08:46:25.861468Z",
     "iopub.status.busy": "2025-07-30T08:46:25.861284Z",
     "iopub.status.idle": "2025-07-30T08:46:29.413382Z",
     "shell.execute_reply": "2025-07-30T08:46:29.412457Z"
    },
    "papermill": {
     "duration": 3.559594,
     "end_time": "2025-07-30T08:46:29.414867",
     "exception": false,
     "start_time": "2025-07-30T08:46:25.855273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ æ‰¾åˆ° 5 ä¸ªfoldæ¨¡å‹\n",
      "âœ… åŠ è½½ /kaggle/input/imu-only-datas/saved_models/fold_1_model.pth\n",
      "âœ… åŠ è½½ /kaggle/input/imu-only-datas/saved_models/fold_2_model.pth\n",
      "âœ… åŠ è½½ /kaggle/input/imu-only-datas/saved_models/fold_3_model.pth\n",
      "âœ… åŠ è½½ /kaggle/input/imu-only-datas/saved_models/fold_4_model.pth\n",
      "âœ… åŠ è½½ /kaggle/input/imu-only-datas/saved_models/fold_5_model.pth\n",
      "â–¶ INFERENCE MODE â€“ loading artefacts from /kaggle/input/cmi3-models-p\n",
      "  model, scaler, pads loaded â€“ ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "SAVED_MODELS = '/kaggle/input/imu-only-datas/saved_models'\n",
    "feature_scaler = joblib.load(f'{SAVED_MODELS}/feature_scaler.joblib')\n",
    "predictor = EnsemblePredictor(SAVED_MODELS, device='cuda')\n",
    "\n",
    "# ================================\n",
    "# Training Pipeline\n",
    "# ================================\n",
    "if TRAIN:\n",
    "    print(\"â–¶ TRAIN MODE â€“ loading dataset â€¦\")\n",
    "    df = pd.read_csv(RAW_DIR / \"train.csv\")\n",
    "\n",
    "    # Label encoding\n",
    "    le = LabelEncoder()\n",
    "    df['gesture_int'] = le.fit_transform(df['gesture'])\n",
    "    np.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\n",
    "\n",
    "    # Feature list\n",
    "    meta_cols = {'gesture', 'gesture_int', 'sequence_type', 'behavior', 'orientation',\n",
    "                 'row_id', 'subject', 'phase', 'sequence_id', 'sequence_counter'}\n",
    "    feature_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "    imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "    tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "    print(f\"  IMU {len(imu_cols)} | TOF/THM {len(tof_cols)} | total {len(feature_cols)} features\")\n",
    "\n",
    "    # Global scaler\n",
    "    scaler = StandardScaler().fit(df[feature_cols].ffill().bfill().fillna(0).values)\n",
    "    joblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\n",
    "\n",
    "    # Build sequences\n",
    "    seq_gp = df.groupby('sequence_id')\n",
    "    X_list, y_list, id_list = [], [], []\n",
    "    for seq_id, seq in seq_gp:\n",
    "        mat = preprocess_sequence(seq, feature_cols, scaler)\n",
    "        X_list.append(mat)\n",
    "        y_list.append(seq['gesture_int'].iloc[0])\n",
    "        id_list.append(seq_id)\n",
    "        # lens.append(len(mat))\n",
    "    \n",
    "    pad_len = PAD_PERCENTILE#int(np.percentile(lens, PAD_PERCENTILE))\n",
    "    print(pad_len)\n",
    "    np.save(EXPORT_DIR / \"sequence_maxlen.npy\", pad_len)\n",
    "    np.save(EXPORT_DIR / \"feature_cols.npy\", np.array(feature_cols))\n",
    "    id_list = np.array(id_list)\n",
    "    X_list_all = pad_sequences_torch(X_list, maxlen=pad_len, padding='pre', truncating='pre')\n",
    "    y_list_all = np.eye(len(le.classes_))[y_list].astype(np.float32)  # One-hot encoding\n",
    "\n",
    "    augmenter = Augment(\n",
    "        p_jitter=0.9844818619033621, sigma=0.03291295776089293, scale_range=(0.7542342630597011,1.1625052821731077),\n",
    "        p_dropout=0.41782786013520684,\n",
    "        p_moda=0.3910622476959722, drift_std=0.0040285239353308015, drift_max=0.3929358950258158    \n",
    "    )\n",
    "\n",
    "EPOCHS = 125\n",
    "if TRAIN:\n",
    "    # Split\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "    models = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(id_list, np.argmax(y_list_all, axis=1))):\n",
    "\n",
    "        train_list= X_list_all[train_idx]\n",
    "        train_y_list= y_list_all[train_idx]\n",
    "        val_list = X_list_all[val_idx]\n",
    "        val_y_list= y_list_all[val_idx]\n",
    "\n",
    "        \n",
    "        # Data loaders\n",
    "        train_dataset = CMI3Dataset(train_list, train_y_list, maxlen, mode=\"train\", imu_dim=len(imu_cols),\n",
    "                                augment=augmenter)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4,drop_last=True)\n",
    "    \n",
    "        val_dataset = CMI3Dataset(val_list, val_y_list, maxlen, mode=\"val\")\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4,drop_last=True)\n",
    "\n",
    "    \n",
    "        # Model\n",
    "        model = TwoBranchModel(maxlen, len(imu_cols), len(tof_cols), \n",
    "                      len(le.classes_)).to(device)\n",
    "        ema = EMA(model, decay=0.999)\n",
    "        # Optimizer and scheduler\n",
    "        optimizer = Adam(model.parameters(), lr=LR_INIT, weight_decay=WD)\n",
    "        \n",
    "        steps_per_epoch = len(train_loader)\n",
    "        nbatch = len(train_loader)\n",
    "        warmup = epochs_warmup * nbatch\n",
    "        nsteps = EPOCHS * nbatch\n",
    "        scheduler = CosineLRScheduler(optimizer,\n",
    "                          warmup_t=warmup, warmup_lr_init=warmup_lr_init, warmup_prefix=True,\n",
    "                          t_initial=(nsteps - warmup), lr_min=lr_min) \n",
    "    \n",
    "        early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "    \n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_best_acc = 0.0\n",
    "        i_scheduler = 0\n",
    "        \n",
    "        # Training loop\n",
    "        print(\"â–¶ Starting training...\")\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train()\n",
    "            train_preds = []\n",
    "            train_targets = []\n",
    "            for X, y in (train_loader):  \n",
    "                X, y = X.float().to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(X)\n",
    "    \n",
    "                loss = -torch.sum(F.log_softmax(logits, dim=1) * y, dim=1).mean()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                ema.update(model)\n",
    "                train_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                train_targets.extend(y.argmax(dim=1).cpu().numpy())\n",
    "                scheduler.step(i_scheduler)\n",
    "                i_scheduler +=1\n",
    "    \n",
    "                train_loss += loss.item()\n",
    "                \n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                val_preds = []\n",
    "                val_targets = []\n",
    "                for X, y in (val_loader):  \n",
    "                    half = BATCH_SIZE // 2         \n",
    "\n",
    "                    x_front = X[:half]               \n",
    "                    x_back  = X[half:].clone()      \n",
    "                    \n",
    "                    x_back[:, :, 7:] = 0.0    \n",
    "                    X = torch.cat([x_front, x_back], dim=0)  # (B, C, T)\n",
    "                    X, y = X.float().to(device), y.to(device)\n",
    "                    \n",
    "                    logits = model(X)\n",
    "                    val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "                    val_targets.extend(y.argmax(dim=1).cpu().numpy())\n",
    "                    \n",
    "                    loss = F.cross_entropy(logits, y)\n",
    "                    val_loss += loss.item()\n",
    "    \n",
    "            train_acc = CompetitionMetric().calculate_hierarchical_f1(\n",
    "                pd.DataFrame({'gesture': le.classes_[train_targets]}),\n",
    "                pd.DataFrame({'gesture': le.classes_[train_preds]}))\n",
    "            val_acc = CompetitionMetric().calculate_hierarchical_f1(\n",
    "                pd.DataFrame({'gesture': le.classes_[val_targets]}),\n",
    "                pd.DataFrame({'gesture': le.classes_[val_preds]}))\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "        models.append(model)\n",
    "        # Save model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'imu_dim': len(imu_cols),\n",
    "            'tof_dim': len(tof_cols),\n",
    "            'n_classes': len(le.classes_),\n",
    "            'pad_len': pad_len\n",
    "        }, EXPORT_DIR / f\"gesture_two_branch_fold{fold}.pth\")\n",
    "        print(f\"fold: {fold} val_all_acc: {val_acc:.4f}\")\n",
    "        print(\"âœ” Training done â€“ artefacts saved in\", EXPORT_DIR)\n",
    "\n",
    "else:\n",
    "    print(\"â–¶ INFERENCE MODE â€“ loading artefacts from\", PRETRAINED_DIR)\n",
    "    feature_cols = np.load(PRETRAINED_DIR / \"feature_cols.npy\", allow_pickle=True).tolist()\n",
    "    pad_len = int(np.load(PRETRAINED_DIR / \"sequence_maxlen.npy\"))\n",
    "    scaler = joblib.load(PRETRAINED_DIR / \"scaler.pkl\")\n",
    "    gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    imu_cols = [c for c in feature_cols if not (c.startswith('thm_') or c.startswith('tof_'))]\n",
    "    tof_cols = [c for c in feature_cols if c.startswith('thm_') or c.startswith('tof_')]\n",
    "\n",
    "    \n",
    "    # Load model\n",
    "    MODELS = [f'gesture_two_branch_fold{i}.pth' for i in range(5)]\n",
    "    \n",
    "    models = []\n",
    "    for path in MODELS:\n",
    "        checkpoint = torch.load(PRETRAINED_DIR / path, map_location=device)\n",
    "        \n",
    "        model = TwoBranchModel(\n",
    "            checkpoint['pad_len'], \n",
    "            checkpoint['imu_dim'], \n",
    "            checkpoint['tof_dim'], \n",
    "            checkpoint['n_classes']\n",
    "            ).to(device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "\n",
    "    print(\"  model, scaler, pads loaded â€“ ready for evaluation\")\n",
    "\n",
    "# Make sure gesture_classes exists in both modes\n",
    "if TRAIN:\n",
    "    gesture_classes = le.classes_\n",
    "gesture_classes = None\n",
    "def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    global gesture_classes\n",
    "    if gesture_classes is None:\n",
    "        gesture_classes = np.load(PRETRAINED_DIR / \"gesture_classes.npy\", allow_pickle=True)\n",
    "\n",
    "    sequence = sequence.to_pandas()\n",
    "    demographics = demographics.to_pandas()\n",
    "    sequence = pd.merge(sequence, demographics, on='subject', how='left')\n",
    "\n",
    "    # is_imu_only if tof_cols have null else : no\n",
    "    tof_cols = [c for c in sequence.columns if c.startswith(\"tof_\")]\n",
    "    is_imu_only = sequence[tof_cols].isnull().all(axis=1).all()\n",
    "\n",
    "    if is_imu_only:\n",
    "        # IMU-only model\n",
    "        sequence = feature_engineering(sequence)\n",
    "        sequence[NUMERICAL_FEATURES] = feature_scaler.transform(sequence[NUMERICAL_FEATURES])\n",
    "        sequence = sequence.groupby(['sequence_id', 'subject']).apply(\n",
    "            lambda df: df[FEATURE_NAMES].to_dict(orient='records'),\n",
    "            include_groups=False,\n",
    "        )\n",
    "        sequence = sequence.iloc[0]\n",
    "        predicted_label = predictor.predict(sequence)\n",
    "        return str(predicted_label)\n",
    "\n",
    "    else:\n",
    "        # full-data model\n",
    "        df_seq = sequence\n",
    "        mat = preprocess_sequence(df_seq, feature_cols, scaler)\n",
    "        pad = pad_sequences_torch([mat], maxlen=pad_len, padding='pre', truncating='pre')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.FloatTensor(pad).to(device)\n",
    "            predictions = []\n",
    "        \n",
    "            for model in models:\n",
    "                model.eval()\n",
    "                logits = model(x)\n",
    "                pred = torch.argmax(logits, dim=1).item()  \n",
    "                predictions.append(pred)\n",
    "        \n",
    "            vote_counts = Counter(predictions)\n",
    "            idx = vote_counts.most_common(1)[0][0]\n",
    "\n",
    "        return str(gesture_classes[idx])\n",
    "\n",
    "# Kaggle competition interface\n",
    "import kaggle_evaluation.cmi_inference_server\n",
    "inference_server = kaggle_evaluation.cmi_inference_server.CMIInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        data_paths=(\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv',\n",
    "            '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv',\n",
    "        )\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    },
    {
     "datasetId": 7771623,
     "sourceId": 12328761,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7794493,
     "sourceId": 12362602,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242954653,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.208686,
   "end_time": "2025-07-30T08:46:32.747324",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-30T08:46:05.538638",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
